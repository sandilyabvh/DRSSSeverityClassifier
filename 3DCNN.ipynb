{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DE2_wQ8ND3N",
        "outputId": "535b99cc-c220-44aa-ad3b-10d54e2cb8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTkE96ThrjJl",
        "outputId": "8ac4fec4-a9b6-4f96-eae2-7048462f152f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (4.65.0)\n",
            "Collecting timm==0.6.12\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (0.15.1+cu118)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (8.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0+cu118)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm==0.6.12->segmentation-models-pytorch) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.25.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16444 sha256=333a0178334e69281fb4de19ddc80fd4e3966858d0f018a3721b5f9a3449db7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/16/24/752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=366ca4915509d3faf3058ed91934b6095d942acff6bcc6f74d606d2224dbb264\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/3b/4e/2f3015f1ab76f34be28e04c4bcee27e8cabfa70d2eadf8bc3b\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, huggingface-hub, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.13.4 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.2 timm-0.6.12\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp03dGRNmVWu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageEnhance\n",
        "import argparse\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from pprint import pprint\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import cv2\n",
        "from skimage.feature import hog\n",
        "import timm\n",
        "import random\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# import torch.multiprocessing as mp\n",
        "# mp.set_start_method('spawn', force=True)\n",
        "\n",
        "LABELS_Severity = {35: 0,\n",
        "                   43: 0,\n",
        "                   47: 1,\n",
        "                   53: 1,\n",
        "                   61: 2,\n",
        "                   65: 2,\n",
        "                   71: 2,\n",
        "                   85: 2}\n",
        "\n",
        "\n",
        "mean = (.1706)\n",
        "std = (.2112)\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize(size=(64,64)),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    #transforms.RandomCrop((224,224), padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize(size=(64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K5roEODmgfN"
      },
      "outputs": [],
      "source": [
        "class OCTDataset(Dataset):\n",
        "    def __init__(self, annot=None, unique_pairs=None, subset='train', transform=None, device='cpu'):\n",
        "        if subset == 'train':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_train.csv\")                      \n",
        "        elif subset == 'val':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_train.csv\")\n",
        "            self.unique_pairs = unique_pairs\n",
        "        elif subset == 'test':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_test.csv\")\n",
        "\n",
        "        # Extract \"Patient_ID\" and \"Week_Num\" columns\n",
        "        # print(\"Before Pairing \", len(self.annot))\n",
        "        self.patient_ids = self.annot[\"Patient_ID\"]\n",
        "        self.week_nums = self.annot[\"Week_Num\"]\n",
        "        self.patient_ids = self.annot[\"Patient_ID\"]\n",
        "        self.annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(self.annot['DRSS'].values)]\n",
        "        self.drss_class = self.annot['Severity_Label']\n",
        "\n",
        "        if subset == 'train':\n",
        "          # Create unique pairs of values\n",
        "          self.unique_pairs = set(zip(self.patient_ids, self.week_nums, self.drss_class))\n",
        "\n",
        "          # Create a list from the set of unique_pairs\n",
        "          unique_pairs_list = list(self.unique_pairs)\n",
        "\n",
        "          # Shuffle the unique_pairs_list\n",
        "          random.shuffle(unique_pairs_list)\n",
        "\n",
        "          # Calculate the index at which to split the list\n",
        "          split_index = int(0.8 * len(unique_pairs_list))\n",
        "\n",
        "          # Split the list into training and validation pairs\n",
        "          self.unique_pairs = unique_pairs_list[:split_index]\n",
        "          self.unique_validation_pairs = unique_pairs_list[split_index:]\n",
        "\n",
        "        elif subset == 'test':\n",
        "          # Create unique pairs of values\n",
        "          self.unique_pairs = set(zip(self.patient_ids, self.week_nums, self.drss_class))\n",
        "\n",
        "        self.root = os.path.expanduser(\"/content/drive/MyDrive/FML_Project/\")\n",
        "        self.transform = transform\n",
        "        self.nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
        "        self.path_list = self.annot['File_Path'].values\n",
        "\n",
        "        self._labels = [pair[2] for pair in self.unique_pairs]\n",
        "        # self._labels = self.annot['Severity_Label'].values\n",
        "        assert len(self.unique_pairs) == len(self._labels)\n",
        "        \n",
        "        max_samples = int(len(self._labels)) #32 #int(len(self._labels)/2)\n",
        "        self.max_samples = max_samples\n",
        "        self.device = device\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Get the Patient_ID and Week_Num from the indexed element in unique_pairs\n",
        "        patient_id, week_num, target = list(self.unique_pairs)[index]\n",
        "        # Filter the annot DataFrame to select rows that match the Patient_ID and Week_Num\n",
        "        filtered_df = self.annot[(self.annot['Patient_ID'] == patient_id) & (self.annot['Week_Num'] == week_num)]\n",
        "        # Extract the file paths from the filtered DataFrame and return them as a list\n",
        "        file_paths = [self.root + file_path for file_path in filtered_df['File_Path'].values.tolist()]\n",
        "\n",
        "        # Fix for directories containing lesser than 49 images:\n",
        "        # Load the images.\n",
        "        images = [Image.open(fp).convert('RGB') for fp in file_paths]\n",
        "        # Check if the number of images is less than 49.\n",
        "        if len(images) < 49:\n",
        "            # Calculate the number of missing images.\n",
        "            missing_images = 49 - len(images)\n",
        "            # Duplicate a random image from the existing images to fill the missing spots.\n",
        "            for _ in range(missing_images):\n",
        "                random_index = random.randint(0, len(images) - 1)\n",
        "                images.append(images[random_index])\n",
        "\n",
        "        # Load all 49 images and stack them into a single tensor\n",
        "        image_sequence = []\n",
        "        for image in images:\n",
        "            img_gray = image.convert(\"L\")\n",
        "            img_gray = np.array(img_gray)  # Convert PIL Image to a numpy array\n",
        "            img_gray = cv2.Canny(img_gray, 200, 500) \n",
        "            if self.transform is not None:\n",
        "                img_transformed = self.transform(Image.fromarray(img_gray))\n",
        "            image_sequence.append(img_transformed)\n",
        "\n",
        "        image_sequence = torch.stack(image_sequence, dim=0)\n",
        "        return (image_sequence, target)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.max_samples is not None:\n",
        "            return min(len(self._labels), self.max_samples)\n",
        "        else:\n",
        "            return len(self._labels)    \n",
        "\n",
        "#ConvLSTM\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(input_channels + hidden_channels, 4 * hidden_channels, kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        hx, cx = hidden\n",
        "        combined = torch.cat((input, hx), 1)\n",
        "        gates = self.conv(combined)\n",
        "\n",
        "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
        "        ingate = torch.sigmoid(ingate)\n",
        "        forgetgate = torch.sigmoid(forgetgate)\n",
        "        cellgate = torch.tanh(cellgate)\n",
        "        outgate = torch.sigmoid(outgate)\n",
        "\n",
        "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
        "        hy = outgate * torch.tanh(cy)\n",
        "\n",
        "        return hy, cy\n",
        "\n",
        "#define the neural network architecture\n",
        "class OCTClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OCTClassifier, self).__init__()        \n",
        "        self.conv_lstm = ConvLSTMCell(3, 64, 3)\n",
        "        self.unet = smp.Unet(\n",
        "            encoder_name=\"resnet34\",\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=64,\n",
        "            classes=3,\n",
        "            activation=None\n",
        "        )\n",
        "        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, channels, height, width)\n",
        "        batch_size, sequence_length, channels, height, width = x.size()\n",
        "\n",
        "        hidden_state = (torch.zeros(batch_size, 64, height, width).to(x.device),\n",
        "                        torch.zeros(batch_size, 64, height, width).to(x.device))\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            hidden_state = self.conv_lstm(x[:, t], hidden_state)\n",
        "\n",
        "        x = self.unet(hidden_state[0])\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Comment the following lines if you want to return logits instead of class labels\n",
        "        # Apply softmax activation\n",
        "        # x = torch.softmax(x, dim=1)\n",
        "\n",
        "        # # Get the class labels\n",
        "        # x = torch.argmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "def create_oversampler(targets):\n",
        "    class_sample_counts = np.bincount(targets)\n",
        "    weights = 1.0 / torch.tensor(class_sample_counts, dtype=torch.float)\n",
        "    sample_weights = weights[targets]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(targets), replacement=True)\n",
        "    return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fonGBhokvQgB",
        "outputId": "2e18f003-34b8-4368-edf2-be8ecc9e882b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: cuda:0\n",
            "Train and Test loader complete\n",
            "396 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "annot_train_prime = \"/content/drive/MyDrive/FML_Project/df_prime_train.csv\"\n",
        "annot_test_prime = \"/content/drive/MyDrive/FML_Project/df_prime_test.csv\"\n",
        "data_root = \"/content/drive/MyDrive/FML_Project/\"\n",
        "\n",
        "#train_class_distribution = count_class_distribution(trainset)\n",
        "#test_class_distribution = count_class_distribution(testset)\n",
        "#print(\"Trainset class distribution:\", train_class_distribution)\n",
        "#print(\"Testset class distribution:\", test_class_distribution)    \n",
        "\n",
        "#set up the device (GPU or CPU)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Found device:', device)\n",
        "\n",
        "#def __init__(self, annot=None, subset='train', transform=None, device='cpu'):\n",
        "trainset = OCTDataset(subset='train', transform=train_transform, device=device)\n",
        "valset = OCTDataset(subset='val', unique_pairs=trainset.unique_validation_pairs, transform=train_transform, device=device)\n",
        "\n",
        "#define the hyperparameters\n",
        "import os\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:100'\n",
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 5 #50 #100 #95 #100 #25 #10 #5\n",
        "\n",
        "oversampler = create_oversampler(trainset._labels)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, sampler=oversampler, num_workers=4)\n",
        "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
        "print('Train and Test loader complete')\n",
        "\n",
        "# for i in range(10):\n",
        "#   print(trainset[i][0].shape)\n",
        "print(len(trainset), len(valset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xubFSKFDmrVp",
        "outputId": "1d03ca73-06c3-48fe-f065-6ebdd547ae0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model definition complete\n",
            "[125 192  79]\n",
            "tensor([1.0560, 0.6875, 1.6709], device='cuda:0')\n",
            "len of trainloader:50\n",
            "Training start for train batch: 0\n"
          ]
        }
      ],
      "source": [
        "#initialize the model and optimizer\n",
        "model = OCTClassifier().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print('Model definition complete')\n",
        "\n",
        "#define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# to handle imbalanced dataset:\n",
        "class_counts = np.bincount(trainset._labels)\n",
        "print(class_counts)\n",
        "total_samples = len(trainset)\n",
        "class_weights = torch.FloatTensor(total_samples / (len(class_counts) * class_counts)).to(device)\n",
        "# class_weights[0] = class_weights[0] * 2\n",
        "# class_weights[2] = class_weights[2] * 4\n",
        "\n",
        "# class_weights = torch.tensor([1.0 / c for c in class_counts], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(class_weights)\n",
        "#exit()\n",
        "\n",
        "#train the model\n",
        "print('len of trainloader:'+str(len(trainloader)))\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    model.train()  # Set the model to training mode\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        print('Training start for train batch: '+str(i))\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "        optimizer.zero_grad()\n",
        "        #print('Before model call for batch: '+str(i))\n",
        "        outputs = model(inputs)\n",
        "        #print('After model call for batch: '+str(i))\n",
        "        # print('outputs:', outputs)\n",
        "        # print('labels:', labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    print(f'Epoch {epoch + 1} | Train Loss: {train_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS5FOJ6tzWkL",
        "outputId": "2591d7cb-e4f1-411e-caf7-a206b6ad49a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "Testing start for batch: 0\n",
            "Testing start for batch: 1\n",
            "Testing start for batch: 2\n",
            "Testing start for batch: 3\n",
            "Testing start for batch: 4\n",
            "Testing start for batch: 5\n",
            "Testing start for batch: 6\n",
            "Testing start for batch: 7\n",
            "Testing start for batch: 8\n",
            "Testing start for batch: 9\n",
            "Testing start for batch: 10\n",
            "Testing start for batch: 11\n",
            "Testing start for batch: 12\n",
            "Testing start for batch: 13\n",
            "Testing start for batch: 14\n",
            "Testing start for batch: 15\n",
            "Testing start for batch: 16\n",
            "Testing start for batch: 17\n",
            "Testing start for batch: 18\n",
            "Testing start for batch: 19\n",
            "Testing start for batch: 20\n",
            "Balanced accuracy: 0.3461538461538462\n",
            "F1 score: 0.08493647488925812\n"
          ]
        }
      ],
      "source": [
        "testset = OCTDataset(subset='test', transform=test_transform, device=device)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "print(len(testloader))\n",
        "#evaluate the model on the test set\n",
        "model.eval()\n",
        "\n",
        "# turn off gradients for evaluation\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "with torch.no_grad():\n",
        "    #for inputs, labels in testloader:\n",
        "    for i, (inputs, labels) in enumerate(testloader):\n",
        "        print('Testing start for batch: '+str(i))\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        # predicted = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        pred_labels.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # true_labels += labels.cpu().numpy().tolist()\n",
        "        # pred_labels += predicted.cpu().numpy().tolist()\n",
        "\n",
        "# compute the balanced accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "# print the balanced accuracy\n",
        "print('Balanced accuracy:', balanced_accuracy)\n",
        "print('F1 score:', f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}