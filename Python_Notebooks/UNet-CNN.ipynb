{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DE2_wQ8ND3N",
        "outputId": "4d1afd35-aa51-40e0-b1ba-faec50ee773c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTkE96ThrjJl",
        "outputId": "1b8a2020-3f0b-4bb8-d896-f37d213f8f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (0.15.1+cu118)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (8.4.0)\n",
            "Collecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (4.65.0)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.6.12\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0+cu118)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm==0.6.12->segmentation-models-pytorch) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (16.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16444 sha256=554dd9c769c9f76b0e28869c6fb1bef60b1a70bbddb4a628ceceba51b243afbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/16/24/752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=949df65086217252034c8f303fa999fa8ba40c9328f153659927db8bc0032f2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/3b/4e/2f3015f1ab76f34be28e04c4bcee27e8cabfa70d2eadf8bc3b\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, huggingface-hub, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.13.4 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.2 timm-0.6.12\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Fp03dGRNmVWu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageEnhance\n",
        "import argparse\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, confusion_matrix\n",
        "import cv2\n",
        "import timm\n",
        "import random\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "LABELS_Severity = {35: 0,\n",
        "                   43: 0,\n",
        "                   47: 1,\n",
        "                   53: 1,\n",
        "                   61: 2,\n",
        "                   65: 2,\n",
        "                   71: 2,\n",
        "                   85: 2}\n",
        "\n",
        "mean = (.1706)\n",
        "std = (.2112)\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2K5roEODmgfN"
      },
      "outputs": [],
      "source": [
        "class OCTDataset(Dataset):\n",
        "    def __init__(self, annot=None, unique_pairs=None, subset='train', transform=None, device='cpu'):\n",
        "        if subset == 'train':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_train.csv\")                      \n",
        "        elif subset == 'val':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_train.csv\")\n",
        "            self.unique_pairs = unique_pairs\n",
        "        elif subset == 'test':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_test.csv\")\n",
        "\n",
        "        # Extract \"Patient_ID\" and \"Week_Num\" columns\n",
        "        self.patient_ids = self.annot[\"Patient_ID\"]\n",
        "        self.week_nums = self.annot[\"Week_Num\"]\n",
        "        self.patient_ids = self.annot[\"Patient_ID\"]\n",
        "        self.annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(self.annot['DRSS'].values)]\n",
        "        self.drss_class = self.annot['Severity_Label']\n",
        "\n",
        "        if subset == 'train':\n",
        "          # Create unique pairs of values\n",
        "          self.unique_pairs = set(zip(self.patient_ids, self.week_nums, self.drss_class))\n",
        "\n",
        "          # Create a list from the set of unique_pairs\n",
        "          unique_pairs_list = list(self.unique_pairs)\n",
        "\n",
        "          # Shuffle the unique_pairs_list\n",
        "          random.shuffle(unique_pairs_list)\n",
        "\n",
        "          # Calculate the index at which to split the list\n",
        "          split_index = int(0.8 * len(unique_pairs_list))\n",
        "\n",
        "          # Split the list into training and validation pairs\n",
        "          self.unique_pairs = unique_pairs_list[:split_index]\n",
        "          self.unique_validation_pairs = unique_pairs_list[split_index:]\n",
        "\n",
        "        elif subset == 'test':\n",
        "          # Create unique pairs of values\n",
        "          self.unique_pairs = set(zip(self.patient_ids, self.week_nums, self.drss_class))\n",
        "\n",
        "        self.root = os.path.expanduser(\"/content/drive/MyDrive/FML_Project/\")\n",
        "        self.transform = transform\n",
        "        self.nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
        "        self.path_list = self.annot['File_Path'].values\n",
        "\n",
        "        self._labels = [pair[2] for pair in self.unique_pairs]\n",
        "        assert len(self.unique_pairs) == len(self._labels)\n",
        "        \n",
        "        max_samples = int(len(self._labels))\n",
        "        self.max_samples = max_samples\n",
        "        self.device = device\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Get the Patient_ID and Week_Num from the indexed element in unique_pairs\n",
        "        patient_id, week_num, target = list(self.unique_pairs)[index]\n",
        "\n",
        "        # Filter the annot DataFrame to select rows that match the Patient_ID and Week_Num\n",
        "        filtered_df = self.annot[(self.annot['Patient_ID'] == patient_id) & (self.annot['Week_Num'] == week_num)]\n",
        "\n",
        "        # Extract the file paths from the filtered DataFrame and return them as a list\n",
        "        file_paths = [self.root + file_path for file_path in filtered_df['File_Path'].values.tolist()]\n",
        "        # Canny edge of each image blended together\n",
        "        fused_image_path = os.path.dirname(file_paths[0])+\"/cn_final.jpg\"\n",
        "        # Canny edge of each image formed as a grid\n",
        "        # fused_image_path = os.path.dirname(file_paths[0])+\"/grid_image_canny.jpg\"\n",
        "        # OCT images formed as a grid\n",
        "        # fused_image_path = os.path.dirname(file_paths[0])+\"/grid_image.jpg\"\n",
        "        # Canny edge of Alpha blended image\n",
        "        # fused_image_path = os.path.dirname(file_paths[0])+\"/ab_edge_image.jpg\"\n",
        "\n",
        "        img = Image.open(fused_image_path)\n",
        "        img_gray = img.convert(\"L\")\n",
        "        if self.transform is not None:\n",
        "            img_sharpened = self.transform(img_gray)\n",
        "\n",
        "        return (img_sharpened, target)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.max_samples is not None:\n",
        "            return min(len(self._labels), self.max_samples)\n",
        "        else:\n",
        "            return len(self._labels)    \n",
        "\n",
        "# Neural network architecture\n",
        "class OCTClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OCTClassifier, self).__init__()\n",
        "        self.unet = smp.Unet(\n",
        "            # encoder_name=\"resnet34\",   # encoder architecture (e.g., resnet34)\n",
        "            encoder_name=\"timm-efficientnet-b7\", # encoder architecture (e.g., EfficientNet-B3)\n",
        "            encoder_weights=\"imagenet\", # pre-trained weights for the encoder\n",
        "            in_channels=3,              # input channels\n",
        "            classes=3,                  # number of output channels\n",
        "        )\n",
        "        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Pooling layer to get the [32, 3] output shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unet(x)               # U-Net forward pass\n",
        "        x = self.avg_pool(x)           # Apply the pooling layer\n",
        "        x = x.view(x.size(0), -1)      # Reshape the tensor to [batch_size, 3]\n",
        "        return x\n",
        "\n",
        "# Oversampler to use weighted random sampling for data loaders\n",
        "def create_oversampler(targets):\n",
        "    class_sample_counts = np.bincount(targets)\n",
        "    weights = 1.0 / torch.tensor(class_sample_counts, dtype=torch.float)\n",
        "    sample_weights = weights[targets]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(targets), replacement=True)\n",
        "    return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Uv_88ga_8F"
      },
      "outputs": [],
      "source": [
        "####################################################################################################### \n",
        "#  Run this code only during Grid Search\n",
        "####################################################################################################### \n",
        "def train_model(trainloader, val_loader, testloader, model, criterion, optimizer, device, num_epochs):\n",
        "    best_val_balanced_accuracy = 0\n",
        "    best_val_balanced_accuracy_epoch_number = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_true_labels = []\n",
        "        with torch.no_grad():\n",
        "            for val_inputs, val_labels in val_loader:\n",
        "                val_inputs = val_inputs.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "                val_outputs = model(val_inputs)\n",
        "                valdn_loss = criterion(val_outputs, val_labels)\n",
        "                val_running_loss += valdn_loss.item()\n",
        "                \n",
        "                _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                val_preds.extend(val_predicted.cpu().numpy())\n",
        "                val_true_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        val_loss = val_running_loss / len(val_loader)\n",
        "        val_balanced_accuracy = balanced_accuracy_score(val_true_labels, val_preds)\n",
        "\n",
        "        if val_balanced_accuracy > best_val_balanced_accuracy:\n",
        "            best_val_balanced_accuracy = val_balanced_accuracy\n",
        "            best_model = model\n",
        "            best_val_balanced_accuracy_epoch_number = epoch + 1\n",
        "\n",
        "    return best_model, best_val_balanced_accuracy, best_val_balanced_accuracy_epoch_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qi1TRkcEMCjV"
      },
      "outputs": [],
      "source": [
        "####################################################################################################### \n",
        "#  After obtaining best model parameters from Grid Search change the exit condition in below code.\n",
        "#  Use this code for final run.\n",
        "####################################################################################################### \n",
        "def train_model(trainloader, val_loader, model, criterion, optimizer, device, num_epochs):\n",
        "    best_balanced_accuracy = 0\n",
        "    best_balanced_accuracy_epoch_number = 0\n",
        "    final_balanced_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_balanced_accuracy = 0.0\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_true_labels = []\n",
        "        with torch.no_grad():\n",
        "            for val_inputs, val_labels in val_loader:\n",
        "                val_inputs = val_inputs.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "                val_outputs = model(val_inputs)\n",
        "                valdn_loss = criterion(val_outputs, val_labels)\n",
        "                val_running_loss += valdn_loss.item()\n",
        "                \n",
        "                _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                val_preds.extend(val_predicted.cpu().numpy())\n",
        "                val_true_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "        # compute the balanced accuracy\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        val_loss = val_running_loss / len(val_loader)\n",
        "        val_balanced_accuracy = balanced_accuracy_score(val_true_labels, val_preds)\n",
        "\n",
        "        print(f'Epoch {epoch + 1} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val Balanced Accuracy: {val_balanced_accuracy:.2f}')\n",
        "    return model, train_loss, val_loss, val_balanced_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fonGBhokvQgB",
        "outputId": "a39e9b64-cc1e-4bed-e6a6-fe35517d5ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "annot_train_prime = \"/content/drive/MyDrive/FML_Project/df_prime_train.csv\"\n",
        "annot_test_prime = \"/content/drive/MyDrive/FML_Project/df_prime_test.csv\"\n",
        "data_root = \"/content/drive/MyDrive/FML_Project/\"\n",
        "\n",
        "#set up the device (GPU or CPU)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Found device:', device)\n",
        "\n",
        "trainset = OCTDataset(subset='train', transform=train_transform, device=device)\n",
        "valset = OCTDataset(subset='val', unique_pairs=trainset.unique_validation_pairs, transform=train_transform, device=device)\n",
        "testset = OCTDataset(subset='test', transform=test_transform, device=device)\n",
        "\n",
        "oversampler = create_oversampler(trainset._labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PSqxxg7c4bf"
      },
      "outputs": [],
      "source": [
        "####################################################################################################### \n",
        "#  Run this code only during Grid Search\n",
        "####################################################################################################### \n",
        "# Define parameter grid for Grid Search\n",
        "batch_sizes = [8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "num_epochs_list = [200]\n",
        "\n",
        "best_balanced_accuracy = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Perform grid search\n",
        "for batch_size in batch_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_epochs in num_epochs_list:\n",
        "            print(f'Training with batch_size={batch_size}, learning_rate={learning_rate}, num_epochs={num_epochs}')\n",
        "            trainloader = DataLoader(trainset, batch_size=batch_size, sampler=oversampler, num_workers=2)\n",
        "            val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
        "            testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)            \n",
        "            model = OCTClassifier().to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            #define the loss function\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # To handle imbalanced dataset:\n",
        "            class_counts = np.bincount(trainset._labels)\n",
        "            print(class_counts)\n",
        "            total_samples = len(trainset)\n",
        "\n",
        "            # Adjust weights in proportion to class distribution in trainset\n",
        "            class_weights = torch.FloatTensor(total_samples / (len(class_counts) * class_counts)).to(device)\n",
        "            class_weights[0] = class_weights[0] * 2\n",
        "            class_weights[2] = class_weights[2] * 4\n",
        "\n",
        "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "            model, val_balanced_accuracy, best_balanced_accuracy_epoch_number = train_model(trainloader, val_loader, testloader, model, criterion, optimizer, device, num_epochs)\n",
        "            if val_balanced_accuracy > best_balanced_accuracy:\n",
        "                best_balanced_accuracy = val_balanced_accuracy\n",
        "                best_params = (batch_size, learning_rate, num_epochs, best_balanced_accuracy_epoch_number)\n",
        "                best_model = model\n",
        "\n",
        "print(f'Best balanced accuracy: {best_balanced_accuracy}')\n",
        "print(f'Best parameters: batch_size={best_params[0]}, learning_rate={best_params[1]}, num_epochs={best_params[2]}, epoch_number={best_params[3]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JhZs071dVUf",
        "outputId": "c11760b0-ca07-4e9d-b944-751d1d2fa1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with batch_size=16, learning_rate=0.0001, num_epochs=100\n",
            "Epoch 1 | Train Loss: 1.080 | Val Loss: 1.006 | Val Balanced Accuracy: 0.35\n",
            "Epoch 2 | Train Loss: 0.886 | Val Loss: 1.062 | Val Balanced Accuracy: 0.34\n",
            "Epoch 3 | Train Loss: 0.658 | Val Loss: 0.813 | Val Balanced Accuracy: 0.38\n",
            "Epoch 4 | Train Loss: 0.508 | Val Loss: 0.870 | Val Balanced Accuracy: 0.46\n",
            "Epoch 5 | Train Loss: 0.454 | Val Loss: 0.869 | Val Balanced Accuracy: 0.47\n",
            "Epoch 6 | Train Loss: 0.399 | Val Loss: 0.905 | Val Balanced Accuracy: 0.45\n",
            "Epoch 7 | Train Loss: 0.372 | Val Loss: 1.003 | Val Balanced Accuracy: 0.48\n",
            "Epoch 8 | Train Loss: 0.357 | Val Loss: 0.786 | Val Balanced Accuracy: 0.47\n",
            "Epoch 9 | Train Loss: 0.256 | Val Loss: 1.015 | Val Balanced Accuracy: 0.44\n",
            "Epoch 10 | Train Loss: 0.272 | Val Loss: 0.710 | Val Balanced Accuracy: 0.57\n",
            "Epoch 11 | Train Loss: 0.282 | Val Loss: 0.693 | Val Balanced Accuracy: 0.52\n",
            "Epoch 12 | Train Loss: 0.285 | Val Loss: 0.933 | Val Balanced Accuracy: 0.51\n",
            "Epoch 13 | Train Loss: 0.251 | Val Loss: 0.725 | Val Balanced Accuracy: 0.54\n",
            "Epoch 14 | Train Loss: 0.221 | Val Loss: 1.022 | Val Balanced Accuracy: 0.51\n",
            "Epoch 15 | Train Loss: 0.205 | Val Loss: 0.882 | Val Balanced Accuracy: 0.51\n",
            "Epoch 16 | Train Loss: 0.226 | Val Loss: 0.886 | Val Balanced Accuracy: 0.60\n",
            "Epoch 17 | Train Loss: 0.199 | Val Loss: 0.750 | Val Balanced Accuracy: 0.56\n",
            "Epoch 18 | Train Loss: 0.199 | Val Loss: 0.604 | Val Balanced Accuracy: 0.61\n",
            "Epoch 19 | Train Loss: 0.172 | Val Loss: 0.759 | Val Balanced Accuracy: 0.54\n",
            "Epoch 20 | Train Loss: 0.186 | Val Loss: 0.855 | Val Balanced Accuracy: 0.55\n",
            "Epoch 21 | Train Loss: 0.162 | Val Loss: 0.775 | Val Balanced Accuracy: 0.60\n",
            "Epoch 22 | Train Loss: 0.150 | Val Loss: 0.707 | Val Balanced Accuracy: 0.55\n",
            "Epoch 23 | Train Loss: 0.163 | Val Loss: 0.907 | Val Balanced Accuracy: 0.61\n",
            "Epoch 24 | Train Loss: 0.182 | Val Loss: 0.854 | Val Balanced Accuracy: 0.57\n",
            "Epoch 25 | Train Loss: 0.145 | Val Loss: 0.736 | Val Balanced Accuracy: 0.57\n",
            "Epoch 26 | Train Loss: 0.116 | Val Loss: 1.107 | Val Balanced Accuracy: 0.58\n",
            "Epoch 27 | Train Loss: 0.103 | Val Loss: 1.126 | Val Balanced Accuracy: 0.62\n",
            "Epoch 28 | Train Loss: 0.154 | Val Loss: 0.755 | Val Balanced Accuracy: 0.56\n",
            "Epoch 29 | Train Loss: 0.136 | Val Loss: 1.130 | Val Balanced Accuracy: 0.53\n",
            "Epoch 30 | Train Loss: 0.152 | Val Loss: 0.732 | Val Balanced Accuracy: 0.56\n",
            "Epoch 31 | Train Loss: 0.102 | Val Loss: 0.813 | Val Balanced Accuracy: 0.64\n",
            "Epoch 32 | Train Loss: 0.135 | Val Loss: 0.569 | Val Balanced Accuracy: 0.61\n",
            "Epoch 33 | Train Loss: 0.118 | Val Loss: 0.885 | Val Balanced Accuracy: 0.66\n",
            "Epoch 34 | Train Loss: 0.115 | Val Loss: 1.118 | Val Balanced Accuracy: 0.64\n",
            "Epoch 35 | Train Loss: 0.163 | Val Loss: 1.032 | Val Balanced Accuracy: 0.54\n",
            "Epoch 36 | Train Loss: 0.208 | Val Loss: 0.777 | Val Balanced Accuracy: 0.57\n",
            "Epoch 37 | Train Loss: 0.125 | Val Loss: 0.972 | Val Balanced Accuracy: 0.63\n",
            "Epoch 38 | Train Loss: 0.091 | Val Loss: 0.835 | Val Balanced Accuracy: 0.59\n",
            "Epoch 39 | Train Loss: 0.082 | Val Loss: 1.025 | Val Balanced Accuracy: 0.59\n",
            "Epoch 40 | Train Loss: 0.102 | Val Loss: 1.082 | Val Balanced Accuracy: 0.59\n",
            "Epoch 41 | Train Loss: 0.150 | Val Loss: 1.217 | Val Balanced Accuracy: 0.59\n",
            "Epoch 42 | Train Loss: 0.081 | Val Loss: 1.107 | Val Balanced Accuracy: 0.64\n",
            "Epoch 43 | Train Loss: 0.069 | Val Loss: 1.067 | Val Balanced Accuracy: 0.57\n",
            "Epoch 44 | Train Loss: 0.065 | Val Loss: 1.113 | Val Balanced Accuracy: 0.58\n",
            "Epoch 45 | Train Loss: 0.145 | Val Loss: 0.824 | Val Balanced Accuracy: 0.57\n",
            "Epoch 46 | Train Loss: 0.109 | Val Loss: 1.194 | Val Balanced Accuracy: 0.56\n",
            "Epoch 47 | Train Loss: 0.114 | Val Loss: 1.191 | Val Balanced Accuracy: 0.62\n",
            "Epoch 48 | Train Loss: 0.087 | Val Loss: 1.111 | Val Balanced Accuracy: 0.52\n",
            "Epoch 49 | Train Loss: 0.063 | Val Loss: 1.076 | Val Balanced Accuracy: 0.55\n",
            "Epoch 50 | Train Loss: 0.050 | Val Loss: 1.024 | Val Balanced Accuracy: 0.57\n",
            "Epoch 51 | Train Loss: 0.122 | Val Loss: 1.309 | Val Balanced Accuracy: 0.57\n",
            "Epoch 52 | Train Loss: 0.123 | Val Loss: 1.321 | Val Balanced Accuracy: 0.57\n",
            "Epoch 53 | Train Loss: 0.055 | Val Loss: 1.318 | Val Balanced Accuracy: 0.57\n",
            "Epoch 54 | Train Loss: 0.052 | Val Loss: 1.287 | Val Balanced Accuracy: 0.63\n",
            "Epoch 55 | Train Loss: 0.061 | Val Loss: 1.117 | Val Balanced Accuracy: 0.63\n",
            "Epoch 56 | Train Loss: 0.044 | Val Loss: 1.128 | Val Balanced Accuracy: 0.62\n",
            "Epoch 57 | Train Loss: 0.043 | Val Loss: 0.971 | Val Balanced Accuracy: 0.65\n",
            "Epoch 58 | Train Loss: 0.050 | Val Loss: 1.093 | Val Balanced Accuracy: 0.58\n",
            "Epoch 59 | Train Loss: 0.048 | Val Loss: 1.023 | Val Balanced Accuracy: 0.63\n",
            "Epoch 60 | Train Loss: 0.030 | Val Loss: 1.062 | Val Balanced Accuracy: 0.60\n",
            "Epoch 61 | Train Loss: 0.030 | Val Loss: 1.296 | Val Balanced Accuracy: 0.65\n",
            "Epoch 62 | Train Loss: 0.027 | Val Loss: 1.430 | Val Balanced Accuracy: 0.62\n",
            "Epoch 63 | Train Loss: 0.045 | Val Loss: 1.495 | Val Balanced Accuracy: 0.64\n",
            "Epoch 64 | Train Loss: 0.078 | Val Loss: 1.014 | Val Balanced Accuracy: 0.56\n",
            "Epoch 65 | Train Loss: 0.100 | Val Loss: 1.142 | Val Balanced Accuracy: 0.59\n",
            "Epoch 66 | Train Loss: 0.060 | Val Loss: 1.117 | Val Balanced Accuracy: 0.60\n",
            "Epoch 67 | Train Loss: 0.041 | Val Loss: 1.069 | Val Balanced Accuracy: 0.61\n",
            "Epoch 68 | Train Loss: 0.044 | Val Loss: 1.537 | Val Balanced Accuracy: 0.57\n",
            "Epoch 69 | Train Loss: 0.061 | Val Loss: 1.181 | Val Balanced Accuracy: 0.54\n",
            "Epoch 70 | Train Loss: 0.065 | Val Loss: 1.301 | Val Balanced Accuracy: 0.59\n",
            "Epoch 71 | Train Loss: 0.061 | Val Loss: 1.246 | Val Balanced Accuracy: 0.60\n",
            "Epoch 72 | Train Loss: 0.057 | Val Loss: 1.334 | Val Balanced Accuracy: 0.59\n",
            "Epoch 73 | Train Loss: 0.047 | Val Loss: 1.146 | Val Balanced Accuracy: 0.63\n",
            "Epoch 74 | Train Loss: 0.051 | Val Loss: 1.438 | Val Balanced Accuracy: 0.56\n",
            "Epoch 75 | Train Loss: 0.055 | Val Loss: 1.071 | Val Balanced Accuracy: 0.57\n",
            "Epoch 76 | Train Loss: 0.043 | Val Loss: 1.564 | Val Balanced Accuracy: 0.61\n",
            "Epoch 77 | Train Loss: 0.027 | Val Loss: 1.881 | Val Balanced Accuracy: 0.52\n",
            "Epoch 78 | Train Loss: 0.018 | Val Loss: 1.446 | Val Balanced Accuracy: 0.57\n",
            "Epoch 79 | Train Loss: 0.021 | Val Loss: 1.653 | Val Balanced Accuracy: 0.56\n",
            "Epoch 80 | Train Loss: 0.028 | Val Loss: 1.599 | Val Balanced Accuracy: 0.52\n",
            "Epoch 81 | Train Loss: 0.029 | Val Loss: 2.206 | Val Balanced Accuracy: 0.55\n",
            "Epoch 82 | Train Loss: 0.026 | Val Loss: 1.723 | Val Balanced Accuracy: 0.52\n",
            "Epoch 83 | Train Loss: 0.020 | Val Loss: 1.766 | Val Balanced Accuracy: 0.61\n",
            "Epoch 84 | Train Loss: 0.027 | Val Loss: 1.640 | Val Balanced Accuracy: 0.57\n",
            "Epoch 85 | Train Loss: 0.031 | Val Loss: 1.857 | Val Balanced Accuracy: 0.56\n",
            "Epoch 86 | Train Loss: 0.028 | Val Loss: 1.529 | Val Balanced Accuracy: 0.59\n",
            "Epoch 87 | Train Loss: 0.016 | Val Loss: 1.628 | Val Balanced Accuracy: 0.52\n",
            "Epoch 88 | Train Loss: 0.028 | Val Loss: 1.572 | Val Balanced Accuracy: 0.59\n",
            "Epoch 89 | Train Loss: 0.023 | Val Loss: 1.126 | Val Balanced Accuracy: 0.61\n",
            "Epoch 90 | Train Loss: 0.022 | Val Loss: 1.608 | Val Balanced Accuracy: 0.64\n",
            "Epoch 91 | Train Loss: 0.013 | Val Loss: 2.265 | Val Balanced Accuracy: 0.58\n",
            "Epoch 92 | Train Loss: 0.035 | Val Loss: 1.456 | Val Balanced Accuracy: 0.64\n",
            "Epoch 93 | Train Loss: 0.018 | Val Loss: 1.870 | Val Balanced Accuracy: 0.64\n",
            "Epoch 94 | Train Loss: 0.016 | Val Loss: 1.311 | Val Balanced Accuracy: 0.63\n",
            "Epoch 95 | Train Loss: 0.033 | Val Loss: 1.025 | Val Balanced Accuracy: 0.64\n",
            "Epoch 96 | Train Loss: 0.094 | Val Loss: 1.051 | Val Balanced Accuracy: 0.64\n",
            "Epoch 97 | Train Loss: 0.130 | Val Loss: 1.267 | Val Balanced Accuracy: 0.57\n",
            "Epoch 98 | Train Loss: 0.166 | Val Loss: 0.944 | Val Balanced Accuracy: 0.59\n",
            "Epoch 99 | Train Loss: 0.072 | Val Loss: 1.291 | Val Balanced Accuracy: 0.56\n",
            "Epoch 100 | Train Loss: 0.041 | Val Loss: 1.092 | Val Balanced Accuracy: 0.62\n"
          ]
        }
      ],
      "source": [
        "# Best model Hyper-parameters obtained from grid search\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 100\n",
        "\n",
        "best_balanced_accuracy = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Run this \n",
        "print(f'Training with batch_size={batch_size}, learning_rate={learning_rate}, num_epochs={num_epochs}')\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, sampler=oversampler, num_workers=2)\n",
        "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "model = OCTClassifier().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# To handle imbalanced dataset:\n",
        "class_counts = np.bincount(trainset._labels)\n",
        "total_samples = len(trainset)\n",
        "\n",
        "# Adjust weights in proportion to class distribution in trainset\n",
        "class_weights = torch.FloatTensor(total_samples / (len(class_counts) * class_counts)).to(device)\n",
        "class_weights[0] = class_weights[0] * 2\n",
        "class_weights[2] = class_weights[2] * 8\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "model, train_loss, val_loss, val_balanced_accuracy = train_model(trainloader, val_loader, model, criterion, optimizer, device, num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/unet_octclassifier_latest.pth\")"
      ],
      "metadata": {
        "id": "H8ioFZ78i3Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS5FOJ6tzWkL",
        "outputId": "8a2ebf62-a6e7-4b0d-abd1-016febc02be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for batch: 0\n",
            "Testing for batch: 1\n",
            "Testing for batch: 2\n",
            "Testing for batch: 3\n",
            "Testing for batch: 4\n",
            "Testing for batch: 5\n",
            "Testing for batch: 6\n",
            "Testing for batch: 7\n",
            "Testing for batch: 8\n",
            "Testing for batch: 9\n",
            "Testing for batch: 10\n",
            "Balanced accuracy: 0.5816583953680728\n",
            "F1 score: 0.5893705052293798\n",
            "Sensitivity for class 0: 0.65\n",
            "Specificity for class 0: 0.72\n",
            "Sensitivity for class 1: 0.57\n",
            "Specificity for class 1: 0.70\n",
            "Sensitivity for class 2: 0.52\n",
            "Specificity for class 2: 0.92\n"
          ]
        }
      ],
      "source": [
        "testset = OCTDataset(subset='test', transform=test_transform, device=device)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "#Load model from pth file corresponding to maximum accuracy obtained from all U-Net based experiments.\n",
        "#This code is commented while trying new methods and only used to lock the model weights once satisfied with trained model.\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/unet_octclassifier_0dot59.pth\"))\n",
        "\n",
        "#evaluate the model on the test set\n",
        "model.eval()\n",
        "\n",
        "# turn off gradients for evaluation\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(testloader):\n",
        "        print('Testing for batch: '+str(i))\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        pred_labels.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# confusion matrix\n",
        "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# compute the specificivity and sensitivity\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "num_classes = conf_mat.shape[0]\n",
        "for i in range(num_classes):\n",
        "    tp = conf_mat[i, i]\n",
        "    fp = np.sum(conf_mat[:, i]) - tp\n",
        "    fn = np.sum(conf_mat[i, :]) - tp\n",
        "    tn = np.sum(conf_mat) - (tp + fp + fn)\n",
        "\n",
        "    sensitivity.append(tp / (tp + fn))\n",
        "    specificity.append(tn / (tn + fp))\n",
        "\n",
        "# compute the balanced accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "# print the balanced accuracy\n",
        "print('Balanced accuracy:', balanced_accuracy)\n",
        "print('F1 score:', f1)\n",
        "for i in range(num_classes):\n",
        "    print(f'Sensitivity for class {i}: {sensitivity[i]:.2f}')\n",
        "    print(f'Specificity for class {i}: {specificity[i]:.2f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}