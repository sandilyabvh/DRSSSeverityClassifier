{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY40JH-plVA7",
        "outputId": "913b597a-b5d3-45c2-b68f-c4e021b46df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageEnhance\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "LABELS_Severity = {35: 0,\n",
        "                   43: 0,\n",
        "                   47: 1,\n",
        "                   53: 1,\n",
        "                   61: 2,\n",
        "                   65: 2,\n",
        "                   71: 2,\n",
        "                   85: 2}\n",
        "\n",
        "def normalize_np(image, mean, std):\n",
        "    grayscale_image = np.array(image.convert('L'))\n",
        "    return (grayscale_image - mean) / std\n",
        "\n",
        "mean = 0.1706\n",
        "std = 0.2112\n",
        "target_size = (224, 224) #(112, 112)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=target_size),\n",
        "    transforms.Lambda(lambda x: normalize_np(x, mean, std)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(size=target_size),\n",
        "    transforms.Lambda(lambda x: normalize_np(x, mean, std)),\n",
        "])"
      ],
      "metadata": {
        "id": "DiAOkZ-dla3l"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OCTDataset(Dataset):\n",
        "    def __init__(self, annot=None, subset='train', transform=None, device='cpu'):\n",
        "        if subset == 'train':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_train.csv\")                      \n",
        "        elif subset == 'test':\n",
        "            self.annot = pd.read_csv(\"/content/drive/MyDrive/FML_Project/df_prime_test.csv\")\n",
        "\n",
        "        # Extract \"Patient_ID\" and \"Week_Num\" columns\n",
        "        self.patient_ids = self.annot[\"Patient_ID\"]\n",
        "        self.week_nums = self.annot[\"Week_Num\"]\n",
        "        self.patient_ids = self.annot[\"Patient_ID\"]\n",
        "        self.annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(self.annot['DRSS'].values)]\n",
        "        self.drss_class = self.annot['Severity_Label']\n",
        "\n",
        "        # Create unique pairs of values\n",
        "        self.unique_pairs = set(zip(self.patient_ids, self.week_nums, self.drss_class))\n",
        "\n",
        "        self.root = os.path.expanduser(\"/content/drive/MyDrive/FML_Project/\")\n",
        "        self.transform = transform\n",
        "        self.nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
        "        self.path_list = self.annot['File_Path'].values\n",
        "\n",
        "        self._labels = [pair[2] for pair in self.unique_pairs]\n",
        "        assert len(self.unique_pairs) == len(self._labels)\n",
        "        \n",
        "        max_samples = int(len(self._labels)) #32 #int(len(self._labels)/2)\n",
        "        self.max_samples = max_samples\n",
        "        self.device = device\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Get the Patient_ID and Week_Num from the indexed element in unique_pairs\n",
        "        patient_id, week_num, target = list(self.unique_pairs)[index]\n",
        "        # Filter the annot DataFrame to select rows that match the Patient_ID and Week_Num\n",
        "        filtered_df = self.annot[(self.annot['Patient_ID'] == patient_id) & (self.annot['Week_Num'] == week_num)]\n",
        "        # Extract the file paths from the filtered DataFrame and return them as a list\n",
        "        file_paths = [self.root + file_path for file_path in filtered_df['File_Path'].values.tolist()]\n",
        "        \n",
        "        # image_path = os.path.dirname(file_paths[0])+\"/fused_image.jpg\"\n",
        "        image_path = os.path.dirname(file_paths[0])+\"/ab_final.png\"\n",
        "        # image_path = os.path.dirname(file_paths[0])+\"/cn_final.jpg\"\n",
        "        # image_path = os.path.dirname(file_paths[0])+\"/grid_image.jpg\"\n",
        "        # image_path = os.path.dirname(file_paths[0])+\"/grid_image_canny.jpg\"\n",
        "\n",
        "        img = Image.open(image_path)\n",
        "        img_gray = img.convert(\"L\")\n",
        "        # Apply image sharpening\n",
        "        sharpness = ImageEnhance.Sharpness(img_gray)\n",
        "        img_sharpened = sharpness.enhance(2.0)  # Adjust the factor (2.0) to control the level of sharpening\n",
        "        if self.transform is not None:\n",
        "            img_sharpened = self.transform(img_sharpened)\n",
        "\n",
        "        return img_sharpened, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.max_samples is not None:\n",
        "            return min(len(self._labels), self.max_samples)\n",
        "        else:\n",
        "            return len(self._labels)"
      ],
      "metadata": {
        "id": "smQeJF4DnkoO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Found device', device)\n",
        "batch_size = 32\n",
        "\n",
        "trainset = OCTDataset(subset='train', transform=train_transform, device=device)\n",
        "testset = OCTDataset(subset='test', transform=test_transform, device=device)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(trainset[1][0].shape)\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mn-jXSfn_1Z",
        "outputId": "64a413a8-1bc5-464c-ef96-a75ae29b456d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device cuda:0\n",
            "(224, 224)\n",
            "495 163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_features_and_apply_pca(loader, n_components=50):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for inputs, targets in loader:\n",
        "        batch_features = inputs.view(inputs.size(0), -1).detach().cpu().numpy()\n",
        "        batch_labels = targets.detach().cpu().numpy()\n",
        "        features.extend(batch_features)\n",
        "        labels.extend(batch_labels)\n",
        "    \n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca_features = pca.fit_transform(np.array(features))\n",
        "    \n",
        "    return pca_features, np.array(labels)\n",
        "\n",
        "train_images, train_labels = flatten_features_and_apply_pca(train_loader)\n",
        "test_images, test_labels = flatten_features_and_apply_pca(test_loader)\n",
        "\n",
        "print(train_images.shape)"
      ],
      "metadata": {
        "id": "qNH3_SfAoI6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb259244-e627-4a28-fbd6-8704f942d1f1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(495, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_images, train_labels)\n",
        "\n",
        "test_preds = knn.predict(test_images)\n",
        "test_acc = balanced_accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "print(\"Test Balanced Accuracy:\", test_acc)\n",
        "print(\"Test F1 Score:\", test_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ00Ex3jBJjz",
        "outputId": "f867e1b9-0cbf-47e0-e513-ae0b0dcaf512"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Balanced Accuracy: 0.4573200992555831\n",
            "Test F1 Score: 0.4983891123298843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "# Calculate the sensitivity (recall) for each class\n",
        "sensitivity = np.diag(cm) / np.sum(cm, axis=1)\n",
        "\n",
        "# Calculate the specificity for each class\n",
        "specificity = np.zeros(3)\n",
        "for i in range(3):\n",
        "    tp_fp = np.sum(cm[i, :])\n",
        "    tp_fn = np.sum(cm[:, i])\n",
        "    tp = cm[i, i]\n",
        "    tn = np.sum(cm) - tp_fp - tp_fn + tp\n",
        "    specificity[i] = tn / (tn + tp_fp - tp)\n",
        "\n",
        "# Calculate the separability (harmonic mean of sensitivity and specificity)\n",
        "separability = 2 * sensitivity * specificity / (sensitivity + specificity)\n",
        "\n",
        "print(\"\\nSensitivity (Recall):\")\n",
        "print(\"Class 0:\", sensitivity[0])\n",
        "print(\"Class 1:\", sensitivity[1])\n",
        "print(\"Class 2:\", sensitivity[2])\n",
        "\n",
        "print(\"\\nSeparability:\")\n",
        "print(\"Class 0:\", separability[0])\n",
        "print(\"Class 1:\", separability[1])\n",
        "print(\"Class 2:\", separability[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6sWDqm-kpAW",
        "outputId": "d682eada-f1ae-4150-d579-481a1aafe9f0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sensitivity (Recall):\n",
            "Class 0: 0.5961538461538461\n",
            "Class 1: 0.55\n",
            "Class 2: 0.22580645161290322\n",
            "\n",
            "Separability:\n",
            "Class 0: 0.6736079674060661\n",
            "Class 1: 0.577861163227017\n",
            "Class 2: 0.35500650195058514\n"
          ]
        }
      ]
    }
  ]
}